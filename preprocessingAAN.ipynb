{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jeman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import regex\n",
    "import json\n",
    "import random\n",
    "from ftfy import fix_text\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_metadata_file_path = './datasets/aan/release/2014/acl-metadata.txt'\n",
    "paper_citation_file_path = './datasets/aan/release/2014/acl.txt'\n",
    "paper_text_dir_path = './datasets/aan/papers_text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_citation_file = open(paper_citation_file_path)\n",
    "references = {}\n",
    "citations = {}\n",
    "\n",
    "for line in paper_citation_file:\n",
    "    source, target = line.split(' ==> ')\n",
    "    target = target[:-1] # remove '\\n'\n",
    "    \n",
    "    if source in references:\n",
    "        references[source].append(target)\n",
    "    else:\n",
    "        references[source] = [target]\n",
    "        \n",
    "    if target in citations:\n",
    "        citations[target].append(source)\n",
    "    else:\n",
    "        citations[target] = [source]\n",
    "\n",
    "paper_citation_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://github.com/chbrown/acl-anthology-network\n",
    "paper_metadata_file = open(paper_metadata_file_path, 'rb')\n",
    "\n",
    "lines = []\n",
    "\n",
    "for bline in paper_metadata_file:\n",
    "    try:\n",
    "        line = bline.decode('UTF-8')\n",
    "        line = fix_text(line)\n",
    "        lines.append(line)\n",
    "    except:\n",
    "        try:\n",
    "            line = bline.decode('ISO-8859-2')\n",
    "            line = fix_text(line)\n",
    "            lines.append(line)\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "\n",
    "paper_metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_metadata = {}\n",
    "current_paper_id = ''\n",
    "\n",
    "# tags = {id, author, title, venue, year}\n",
    "\n",
    "for line in lines:\n",
    "    result = re.match('(\\w+)\\s+=\\s+\\{\\s*(.*?)\\s*\\}', line)\n",
    "    if result:\n",
    "        tag = result[1]\n",
    "        value = result[2]\n",
    "        \n",
    "        if tag == 'id':\n",
    "            current_paper_id = value\n",
    "            paper_metadata[current_paper_id] = {}\n",
    "        \n",
    "        paper_metadata[current_paper_id][tag] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5fa0e1f6904e47b1562c1cb0202bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23595.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19197"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file extensions = {body, cite, ref, tmp, txt}, only need txt\n",
    "textfiles = [f for f in os.listdir(paper_text_dir_path) if '.txt' in f]\n",
    "\n",
    "paper_texts = {}\n",
    "\n",
    "for filename in tqdm(textfiles):\n",
    "    paper_id = filename.split('.')[0]\n",
    "    path = paper_text_dir_path + '/' + filename\n",
    "    \n",
    "    with open(path) as file:\n",
    "        text = ''\n",
    "        abstract_len = 0\n",
    "        begin_abstract = False\n",
    "        end_abstract = False\n",
    "        for line in file:\n",
    "            if begin_abstract:\n",
    "                if regex.search('(introduction){e<=3}\\s*$', line.lower()):\n",
    "                    begin_abstract = False\n",
    "                    end_abstract = True\n",
    "                    break\n",
    "                elif regex.search('^\\s*[0|1]', line):\n",
    "                    begin_abstract = False\n",
    "                    end_abstract = True\n",
    "                    break\n",
    "                elif regex.search('(biographies){e<=3}\\s*$', line.lower()):\n",
    "                    begin_abstract = False\n",
    "                    end_abstract = True\n",
    "                    break\n",
    "                elif regex.search('(1\\.?(introduction){e<=3})', line.lower()):\n",
    "                    str_len = len(regex.search('^(.+)(1\\.?(introduction){e<=3})', line.lower())[1])\n",
    "                    text += line[:str_len]\n",
    "                    abstract_len += 1\n",
    "                    begin_abstract = False\n",
    "                    end_abstract = True\n",
    "                    break\n",
    "                else:\n",
    "                    text += line[:-1]\n",
    "                    abstract_len += 1\n",
    "            elif regex.search('(abstract){e<=1}\\s*$', line.lower()):\n",
    "                begin_abstract = True\n",
    "                \n",
    "    if text and end_abstract:\n",
    "        paper_texts[paper_id] = text\n",
    "\n",
    "len(paper_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reject abstracts which contain mail or phone number\n",
    "rejected_abstracts = []\n",
    "\n",
    "for key, value in paper_texts.items():\n",
    "    if re.search('[\\w|\\-]+@[\\w|\\-]+\\.[a-zA-Z]+|[0-9]+[\\-\\s]+[0-9]+[\\-\\s]+[0-9]+', value):\n",
    "        rejected_abstracts.append(key)\n",
    "        \n",
    "for key in rejected_abstracts:\n",
    "    paper_texts.pop(key)\n",
    "    \n",
    "len(paper_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14c4a39b9a348c1b21761da8b1e4371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in tqdm(paper_texts.items()):\n",
    "    corrected_value = value.strip()\n",
    "    \n",
    "    p = re.compile('[\\s|\\t]+')\n",
    "    corrected_value = p.sub(' ', corrected_value)\n",
    "    \n",
    "    p = re.compile('\\s*\\-\\s*')\n",
    "    corrected_value = p.sub('-', corrected_value)\n",
    "    \n",
    "    for match in re.findall('(([a-zA-Z]+)\\-([a-zA-Z]+))', corrected_value):\n",
    "        if match[1] + match[2] in words.words():\n",
    "            corrected_value = corrected_value.replace(match[0],\n",
    "                                                      match[1] + match[2])\n",
    "    \n",
    "    # similarly removing spaces between words and checking if english word\n",
    "    # to correct parsing errors is tempting but it generates some errors\n",
    "    # and is very long to process\n",
    "    # examples : 'to a' --> 'toa', 'and a' --> 'anda', 'a key' --> 'akey'\n",
    "    \n",
    "    paper_texts[key] = corrected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper explores the problem of identifying sen- tence boundaries in the transcriptions produced by automatic speech recognition systems. An experi- ment which determines the level of human perform- ance for this task is described as well as a memory- based computational pproach to the problem. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_texts['A00-1012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper explores the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems. An experiment which determines the level of human performance for this task is described as well as a memory-based computational pproach to the problem.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_texts['A00-1012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work is in the context of TRANSTYPE, a sys- tem that observes its user as he or she types a trans- lation and repeatedly suggests completions for the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a pro- totype which suggests completions of units of texts that are longer than one word. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_texts['A00-1019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This work is in the context of TRANSTYPE, a system that observes its user as he or she types a translation and repeatedly suggests completions for the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a prototype which suggests completions of units of texts that are longer than one word.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_texts['A00-1019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hans  Pau lussen  Facult6s Universitaires Notre-Dame de la Paix, rue de Bruxelles 61, B-5000 Namur, Belgium phone +32-81-72.41.37, fax +32-81-23.03.91, e-mail hpaulus@cc.fundp.ac.be Wi l ly  Mar t in  Vrije Universiteit, De Boelelaan 1105, NL- 1007 MC Amsterdam, The Netherlands phone +31-20-548.37.63, fax +31-20-661.30.54, e-mail lexico@let.vu.nl '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_texts['A92-1019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'A92-1019'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8a83ac77866c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpaper_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A92-1019'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'A92-1019'"
     ]
    }
   ],
   "source": [
    "paper_texts['A92-1019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_id in paper_metadata.keys():\n",
    "    if paper_id in citations:\n",
    "        paper_metadata[paper_id]['citations'] = citations[paper_id]\n",
    "    else:\n",
    "        paper_metadata[paper_id]['citations'] = []\n",
    "    \n",
    "    if paper_id in references:\n",
    "        paper_metadata[paper_id]['references'] = references[paper_id]\n",
    "    else:\n",
    "        paper_metadata[paper_id]['references'] = []\n",
    "        \n",
    "    if paper_id in paper_texts:\n",
    "        paper_metadata[paper_id]['abstract'] = paper_texts[paper_id]\n",
    "    else:\n",
    "        paper_metadata[paper_id]['abstract'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter paper without complete metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b32f2e27ff44a269a84db3fa30b1f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23775.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16391"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags = {id, author, title, venue, year}\n",
    "filtered_papers = []\n",
    "filtered_ids = set()\n",
    "\n",
    "for paper in tqdm(paper_metadata.values()):\n",
    "    ok = True\n",
    "    \n",
    "    if 'id' not in paper or not paper['id']:\n",
    "        ok = False\n",
    "        \n",
    "    if 'author' not in paper or not paper['author']:\n",
    "        ok = False\n",
    "    else:\n",
    "        paper['author'] = [aut.strip() for aut in paper['author']]\n",
    "        \n",
    "    if 'title' not in paper or not paper['title']:\n",
    "        ok = False\n",
    "    else:\n",
    "        paper['title'] = paper['title'].strip()\n",
    "    \n",
    "    if 'venue' not in paper or not paper['venue']:\n",
    "        paper['venue'] = ''\n",
    "    else:\n",
    "        paper['venue'] = paper['venue'].strip()\n",
    "        \n",
    "    if 'year' not in paper or not paper['year'] or not paper['year'].isdigit():\n",
    "        ok = False\n",
    "    else:\n",
    "        paper['year'] = int(paper['year'])\n",
    "        \n",
    "    if not paper['abstract']:\n",
    "        ok = False\n",
    "    else:\n",
    "        paper['abstract'] = paper['abstract'].strip()\n",
    "        \n",
    "    if not paper['citations'] and not paper['references']:\n",
    "        ok = False\n",
    "        \n",
    "    if ok:\n",
    "        filtered_papers.append(paper)\n",
    "        filtered_ids.add(paper['id'])\n",
    "        \n",
    "len(filtered_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in  filtered_papers:\n",
    "    paper['references'] = [paper_id for paper_id in paper['references'] if paper_id in filtered_ids]\n",
    "    paper['citations'] = [paper_id for paper_id in paper['citations'] if paper_id in filtered_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_aan.json', 'w') as f:\n",
    "    json.dump(filtered_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepro part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New problems identified during data analyzing (dist plot of features):\n",
    "- too long / too short abstracts\n",
    "- no link (in/out citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16391"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aan_dataset_path = \"./datasets/preprocessed_aan.json\"\n",
    "with open(aan_dataset_path) as f:\n",
    "    aan_dataset = json.load(f)\n",
    "len(aan_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too short abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = []\n",
    "\n",
    "for ref in aan_dataset:\n",
    "    if not (len(ref['abstract'].split()) < 20 and ref['abstract'][-1] != '.'):\n",
    "        cleaned_dataset.append(ref)\n",
    "        \n",
    "aan_dataset = cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16318"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too long abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = []\n",
    "\n",
    "for ref in aan_dataset:\n",
    "    if len(ref['abstract'].split()) < 500:\n",
    "        cleaned_dataset.append(ref)\n",
    "        \n",
    "aan_dataset = cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refs without in/out citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_links(references):\n",
    "    ids = set([ref['id'] for ref in references])\n",
    "    \n",
    "    recomputed_refs = []\n",
    "    \n",
    "    for ref in references:\n",
    "        ref['references'] = [i for i in ref['references'] if i in ids]\n",
    "        ref['citations'] = [i for i in ref['citations'] if i in ids]\n",
    "        recomputed_refs.append(ref)\n",
    "        \n",
    "    return recomputed_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_link(references):\n",
    "    cleaned_dataset = []\n",
    "    \n",
    "    for ref in references:\n",
    "        if ref['references'] or ref['citations']:\n",
    "            cleaned_dataset.append(ref)\n",
    "    \n",
    "    print('remove_no_link -->',str(len(cleaned_dataset)-len(references)),'removed')\n",
    "    \n",
    "    return cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_no_link --> -532 removed\n",
      "remove_no_link --> 0 removed\n"
     ]
    }
   ],
   "source": [
    "old_len = len(aan_dataset)\n",
    "cleaned_dataset = remove_no_link(recompute_links(aan_dataset))\n",
    "cleaned_len = len(cleaned_dataset)\n",
    "\n",
    "while old_len > cleaned_len:\n",
    "    old_len = cleaned_len\n",
    "    cleaned_dataset = remove_no_link(recompute_links(cleaned_dataset))\n",
    "    cleaned_len = len(cleaned_dataset)\n",
    "    \n",
    "aan_dataset = cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15602"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct author list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in aan_dataset:\n",
    "    ref['author'] = \"\".join(ref['author']).split(';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to json 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed2_aan.json', 'w') as f:\n",
    "    json.dump(aan_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepro part 3\n",
    "\n",
    "Only keeping the big connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15602"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aan_dataset_path = \"./datasets/preprocessed2_aan.json\"\n",
    "with open(aan_dataset_path) as f:\n",
    "    aan_dataset = json.load(f)\n",
    "len(aan_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15602"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for paper in aan_dataset:\n",
    "    for ref_id in paper['references']:\n",
    "        G.add_edge(paper['id'], ref_id)\n",
    "    for cit_id in paper['citations']:\n",
    "        G.add_edge(cit_id, paper['id'])\n",
    "        \n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15366"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggest_cc = max(nx.connected_components(G), key=len)\n",
    "prepro_aan_dataset = [ref for ref in aan_dataset if ref['id'] in biggest_cc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed3_aan.json', 'w') as f:\n",
    "    json.dump(prepro_aan_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15366"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for paper in aan_dataset:\n",
    "    for ref_id in paper['references']:\n",
    "        G.add_edge(paper['id'], ref_id)\n",
    "    for cit_id in paper['citations']:\n",
    "        G.add_edge(cit_id, paper['id'])\n",
    "        \n",
    "len(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepro part 4\n",
    "\n",
    "Add fos from DBLP v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15366"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_dataset_path = './datasets/dblp-aminer_v12/dblp.v12.json'\n",
    "N_RECORDS = 4894081\n",
    "\n",
    "aan_dataset_path = \"./datasets/preprocessed3_aan.json\"\n",
    "with open(aan_dataset_path) as f:\n",
    "    aan_dataset = json.load(f)\n",
    "    \n",
    "aan_dataset_dict = {record['id']: record for record in aan_dataset}\n",
    "\n",
    "len(aan_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932bf97a884740b69fc6bf6f25723d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4894081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12636"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_set = set()\n",
    "titles = {record['title'].lower() for record in aan_dataset}\n",
    "title_id_dict = {record['title'].lower():record['id'] for record in aan_dataset}\n",
    "\n",
    "with open(dblp_dataset_path) as file:\n",
    "    file.readline() # skip first line\n",
    "    for _ in tqdm(range(N_RECORDS)):\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            record = json.loads(line[1:])\n",
    "        except:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "        if record['title'].lower() in titles and 'fos' in record:\n",
    "            paper_id = title_id_dict[record['title'].lower()]\n",
    "            ids_set.add(paper_id)\n",
    "            aan_dataset_dict[paper_id]['fos'] = [fos['name'] for fos in record['fos']]\n",
    "            \n",
    "len(ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12339"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations = nx.DiGraph()\n",
    "\n",
    "for record in aan_dataset:\n",
    "    if record['id'] in ids_set and 'references' in record:\n",
    "        for ref_id in record['references']:\n",
    "            if ref_id in ids_set:\n",
    "                citations.add_edge(record['id'],ref_id)\n",
    "            \n",
    "len(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12274"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undirected_citations = citations.to_undirected()\n",
    "largest_cc = max(nx.connected_components(undirected_citations), key=len)\n",
    "len(largest_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_to_remove = [node for node in citations if node not in largest_cc]\n",
    "citations.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "len(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12274"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_records = [record for record in aan_dataset_dict.values() if record['id'] in largest_cc]\n",
    "    \n",
    "len(selected_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in selected_records:\n",
    "    record['references'] = list(citations.successors(record['id']))\n",
    "    record['citations'] = list(citations.predecessors(record['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed4_aan.json', 'w') as file:\n",
    "    json.dump(selected_records, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepro part 5\n",
    "\n",
    "Add fos weights from DBLP v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_dataset_path = './datasets/dblp-aminer_v12/dblp.v12.json'\n",
    "N_RECORDS = 4894081\n",
    "\n",
    "aan_dataset_path = \"./datasets/preprocessed4_aan.json\"\n",
    "with open(aan_dataset_path) as f:\n",
    "    aan_dataset = json.load(f)\n",
    "    \n",
    "aan_dataset_dict = {record['title'].lower():record for record in aan_dataset}\n",
    "\n",
    "len(aan_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78111b994074269b06a4f2c9abc4b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4894081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12274"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_set = set()\n",
    "\n",
    "with open(dblp_dataset_path) as file:\n",
    "    file.readline() # skip first line\n",
    "    for _ in tqdm(range(N_RECORDS)):\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            record = json.loads(line[1:])\n",
    "        except:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "        if record['title'].lower() in aan_dataset_dict and 'fos' in record:\n",
    "            titles_set.add(record['title'].lower())\n",
    "            aan_dataset_dict[record['title'].lower()]['fos_w'] = record['fos']\n",
    "            \n",
    "len(titles_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_prob = 0\n",
    "for record in aan_dataset_dict.values():\n",
    "    fos_list = record['fos']\n",
    "    fos_w_list = [fos['name'] for fos in record['fos_w']]\n",
    "    \n",
    "    if not collections.Counter(fos_list) == collections.Counter(fos_w_list):\n",
    "        nb_prob += 1  \n",
    "        \n",
    "nb_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed4_aan_with_fos_w.json', 'w') as file:\n",
    "    json.dump(aan_dataset, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
