{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir_path = './datasets/dblp-aminer_v12/'\n",
    "dataset_path = './datasets/dblp-aminer_v12/dblp.v12.json'\n",
    "N_RECORDS = 4894081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_abstract(inverted_index):    \n",
    "    index = {}\n",
    "    for word, list_of_pos in inverted_index.items():\n",
    "        for pos in list_of_pos:\n",
    "            index[pos] = word\n",
    "    \n",
    "    abstract_list = []\n",
    "    for _, word in sorted(index.items(), key=lambda t: t[0]):\n",
    "        abstract_list.append(word)\n",
    "    \n",
    "    return \" \".join(abstract_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : keyword-based selection (Deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932012a768e94b2a8b71b2dc72a752f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4894081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35878"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_records = []\n",
    "\n",
    "with open(dataset_path) as file:\n",
    "    file.readline() # skip first line\n",
    "    for _ in tqdm(range(N_RECORDS)):\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            record = json.loads(line[1:])\n",
    "        except:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "        if 'fos' in record:\n",
    "            for fos in record['fos']:\n",
    "                if fos['name'].lower() == 'deep learning':\n",
    "                    selected_records.append(record)\n",
    "                    break\n",
    "        \n",
    "len(selected_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : remove records with incomplete attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b389098b849c469ba0a801f75acbc058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29327"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_remove = set()\n",
    "attributes = ['title', 'authors', 'venue', 'year',\n",
    "              'references', 'indexed_abstract']\n",
    "\n",
    "for record in tqdm(selected_records):    \n",
    "    if not all(attribute in record for attribute in attributes):\n",
    "        ids_to_remove.add(record['id'])\n",
    "        \n",
    "selected_records = [record for record in selected_records if record['id'] not in ids_to_remove]\n",
    "    \n",
    "len(selected_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d4be98a61b4d54adf4a4dd3680ef62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27307"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_remove = set()\n",
    "\n",
    "for record in tqdm(selected_records):\n",
    "    remove = False\n",
    "    if len(record['title'].split()) > 100:\n",
    "        remove = True            \n",
    "    if 'id' not in record['venue'] or not record['venue']['id']:\n",
    "        remove = True\n",
    "    if 'raw' not in record['venue'] or not record['venue']['raw']:\n",
    "        remove = True\n",
    "    if 'InvertedIndex' not in record['indexed_abstract']:\n",
    "        remove = True\n",
    "    else:\n",
    "        abstract = reconstruct_abstract(record['indexed_abstract']['InvertedIndex'])\n",
    "        if len(abstract.split()) > 500:\n",
    "            remove = True\n",
    "        if len(abstract.split()) < 20 and abstract[-1] != '.':\n",
    "            remove = True\n",
    "    if record['authors']:\n",
    "        nb_valid_authors = 0\n",
    "        for author in record['authors']:\n",
    "            if 'name' in author and author['name'] and 'id' in author and author['id']:\n",
    "                nb_valid_authors += 1\n",
    "        if not nb_valid_authors:\n",
    "            remove = True\n",
    "    else:\n",
    "        remove = True\n",
    "\n",
    "    if remove:\n",
    "        ids_to_remove.add(record['id'])\n",
    "        \n",
    "selected_records = [record for record in selected_records if record['id'] not in ids_to_remove]\n",
    "    \n",
    "len(selected_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : remove records with no reference and no citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23089"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations = nx.DiGraph()\n",
    "record_ids_set = {record['id'] for record in selected_records}\n",
    "\n",
    "for record in selected_records:\n",
    "    if 'references' in record:\n",
    "        for ref_id in record['references']:\n",
    "            if ref_id in record_ids_set:\n",
    "                citations.add_edge(record['id'],ref_id)\n",
    "            \n",
    "len(restrict_citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22726"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undirected_citations = citations.to_undirected()\n",
    "largest_cc = max(nx.connected_components(undirected_citations), key=len)\n",
    "len(largest_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22726"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_records = [record for record in selected_records if record['id'] in largest_cc]\n",
    "    \n",
    "len(selected_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22726"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_to_remove = [node for node in citations if node not in largest_cc]\n",
    "citations.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "len(citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : format metadata and save subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22726"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_records = []\n",
    "\n",
    "for record in selected_records:\n",
    "    formatted_record = {}\n",
    "    \n",
    "    formatted_record['id'] = record['id']\n",
    "    formatted_record['title'] = record['title']\n",
    "    formatted_record['authors'] = record['authors']\n",
    "    formatted_record['year'] = record['year']\n",
    "    formatted_record['venue'] = record['venue']\n",
    "    formatted_record['fos'] = [fos['name'] for fos in record['fos']]\n",
    "    formatted_record['abstract'] = reconstruct_abstract(record['indexed_abstract']['InvertedIndex'])\n",
    "    formatted_record['references'] = list(citations.successors(record['id']))\n",
    "    formatted_record['citations'] = list(citations.predecessors(record['id']))\n",
    "    \n",
    "    formatted_records.append(formatted_record)\n",
    "    \n",
    "len(formatted_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_dblp_v12_subset.json', 'w') as file:\n",
    "    json.dump(formatted_records, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : adding fos weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22726"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_dblp_dataset_path = './datasets/dblp-aminer_v12/dblp.v12.json'\n",
    "N_RECORDS = 4894081\n",
    "\n",
    "dblp_dataset_path = './datasets/preprocessed_dblp_v12_subset.json'\n",
    "with open(dblp_dataset_path) as f:\n",
    "    dblp_dataset = json.load(f)\n",
    "    \n",
    "dblp_dataset_dict = {record['id']:record for record in dblp_dataset}\n",
    "\n",
    "len(dblp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f8ceaf9cae4b2890215ae2a72c6e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4894081.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22726"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_set = set()\n",
    "\n",
    "with open(complete_dblp_dataset_path) as file:\n",
    "    file.readline() # skip first line\n",
    "    for _ in tqdm(range(N_RECORDS)):\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            record = json.loads(line[1:])\n",
    "        except:\n",
    "            record = json.loads(line)\n",
    "            \n",
    "        if record['id'] in dblp_dataset_dict and 'fos' in record:\n",
    "            ids_set.add(record['id'])\n",
    "            dblp_dataset_dict[record['id']]['fos_w'] = record['fos']\n",
    "            \n",
    "len(ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_prob = 0\n",
    "for record in dblp_dataset_dict.values():\n",
    "    fos_list = record['fos']\n",
    "    fos_w_list = [fos['name'] for fos in record['fos_w']]\n",
    "    \n",
    "    if not collections.Counter(fos_list) == collections.Counter(fos_w_list):\n",
    "        nb_prob += 1  \n",
    "        \n",
    "nb_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_dblp_v12_subset_with_fos_w.json', 'w') as file:\n",
    "    json.dump(dblp_dataset, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 : convert int ids to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_dblp_v12_subset_with_fos_w.json') as file:\n",
    "    papers = json.load(file)\n",
    "    \n",
    "    for paper in papers:\n",
    "        paper['id'] = str(paper['id'])\n",
    "        paper['citations'] = [str(cit) for cit in paper['citations']]\n",
    "        paper['references'] = [str(ref) for ref in paper['references']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_dblp_v12_subset_with_fos_w.json', 'w') as file:\n",
    "    json.dump(papers, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_dblp_v12_subset.json') as file:\n",
    "    papers = json.load(file)\n",
    "    \n",
    "    for paper in papers:\n",
    "        paper['id'] = str(paper['id'])\n",
    "        paper['citations'] = [str(cit) for cit in paper['citations']]\n",
    "        paper['references'] = [str(ref) for ref in paper['references']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/preprocessed_dblp_v12_subset.json', 'w') as file:\n",
    "    json.dump(papers, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
