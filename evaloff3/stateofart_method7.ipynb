{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 7 : DiSCern: A diversified citation recommendation system for scientific queries (2015)\n",
    "\n",
    "Source : Chakraborty, T., Modani, N., Narayanam, R., & Nagar, S. (2015). DiSCern: A diversified citation recommendation system for scientific queries. In 2015 IEEE 31st International Conference on Data Engineering (Vol. 2015–May, pp. 555–566)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import json\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import community as louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_SET_PATH = './data/aan_full.json'\n",
    "TEST_SET_PATH = './data/aan_test.json'\n",
    "LOC_OUTPUT_PATH = './results/stateofart_method7_loc_aan.json'\n",
    "GLO_OUTPUT_PATH = './results/stateofart_method7_glo_aan.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_SET_PATH = './data/dblp_full.json'\n",
    "TEST_SET_PATH = './data/dblp_test.json'\n",
    "LOC_OUTPUT_PATH = './results/stateofart_method7_loc_dblp.json'\n",
    "GLO_OUTPUT_PATH = './results/stateofart_method7_glo_dblp.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FULL_SET_PATH) as f:\n",
    "    full_set = json.load(f)\n",
    "    \n",
    "full_set_dict = dict([(paper['id'], paper) for paper in full_set])\n",
    "\n",
    "keyword_paper_dict = {}\n",
    "for paper in full_set:\n",
    "    for fos in paper['fos']:\n",
    "        if fos not in keyword_paper_dict:\n",
    "            keyword_paper_dict[fos] = [paper['id']]\n",
    "        else:\n",
    "            keyword_paper_dict[fos].append(paper['id'])\n",
    "\n",
    "with open(TEST_SET_PATH) as f:\n",
    "    test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Citation network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_graph = nx.DiGraph()\n",
    "for paper in full_set:\n",
    "    citation_graph.add_edge(paper['id'], paper['id'])\n",
    "    for ref_id in paper['references']:\n",
    "        citation_graph.add_edge(paper['id'], ref_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Keyword network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_graph = nx.Graph()\n",
    "keyword_graph.add_nodes_from(keyword_paper_dict.keys())\n",
    "\n",
    "\n",
    "for paper in full_set:\n",
    "    keywords = paper['fos'].copy()\n",
    "    \n",
    "    while keywords:\n",
    "        current_keyword = keywords.pop()\n",
    "        for other_keyword in keywords:\n",
    "            if keyword_graph.has_edge(current_keyword, other_keyword):\n",
    "                keyword_graph[current_keyword][other_keyword]['weight'] += 1\n",
    "            else:\n",
    "                keyword_graph.add_edge(current_keyword, other_keyword, weight=1)\n",
    "                \n",
    "# create communities\n",
    "keyword_community_dict = louvain.best_partition(keyword_graph)\n",
    "community_keyword_dict = dict([(community, []) for community in set(keyword_community_dict.values())])\n",
    "for keyword, community  in keyword_community_dict.items():\n",
    "    community_keyword_dict[community].append(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Query expansion by clustering keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(G, input_papers):\n",
    "    input_keywords = set()\n",
    "    for paper in input_papers:\n",
    "        input_keywords.update(paper['fos'])\n",
    "        \n",
    "    communities = set([keyword_community_dict[keyword] for keyword in input_keywords])\n",
    "        \n",
    "    expand_keywords = set()\n",
    "    for community in communities:\n",
    "        expand_keywords.update(community_keyword_dict[community])\n",
    "                \n",
    "    expand_papers = set()\n",
    "    for keyword in expand_keywords:\n",
    "        expand_papers.update(keyword_paper_dict[keyword])\n",
    "        \n",
    "    return G.subgraph(expand_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Retrieving diverse and relevant citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(G, input_papers, alpha_c = 0.25, lambda_c = 0.9, n_iter = 100, limit = 100):\n",
    "    input_paper_ids = set([p['id'] for p in input_papers])\n",
    "    p_star = 1/G.number_of_nodes()\n",
    "    \n",
    "    p_0_uv = {}\n",
    "    for u in G.nodes():\n",
    "        p_0_uv[u] = {}\n",
    "        node_degree = G.out_degree(u)\n",
    "        for v in G.neighbors(u):\n",
    "            p_0_uv[u][v] = alpha_c / node_degree\n",
    "        p_0_uv[u][u] = 1 - alpha_c\n",
    "    \n",
    "    # init for first iter\n",
    "    p_t_u = dict([(node, p_star) for node in G.nodes()])\n",
    "    p_t_uv = p_0_uv\n",
    "    for _ in range(20):\n",
    "        d_t_u = {}\n",
    "        for u in G.nodes():\n",
    "            d_t_u[u] = sum([p_0_uv[u][v] * p_t_u[v] for v in G.neighbors(u)])\n",
    "        \n",
    "        p_t1_u = {}\n",
    "        for u in G.nodes():\n",
    "            p_t1_u[u] = sum([p_t_uv[u][v] * p_t_u[u] for v in G.neighbors(u)])\n",
    "        \n",
    "        \n",
    "        p_t1_uv = {}\n",
    "        for u in G.nodes():\n",
    "            p_t1_uv[u] = {}\n",
    "            for v in G.neighbors(u):\n",
    "                p_t1_uv[u][v] = (1-lambda_c) * p_star + lambda_c * p_0_uv[u][v] * p_t_u[v] / d_t_u[u]\n",
    "                \n",
    "        p_t_u = p_t1_u\n",
    "        p_t_uv = p_t1_uv\n",
    "    \n",
    "    candidate_scores = [(p, v) for p, v in p_t_u.items() if p not in input_paper_ids]\n",
    "    candidate_scores.sort(key=lambda e: e[1], reverse=True)\n",
    "    limit = min(limit, len(candidate_scores))\n",
    "    \n",
    "    return [e[0] for e in candidate_scores[:limit]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Compute recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f3c3eed4484f64a2e240259485414f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LocDiSCern\n",
    "results = []\n",
    "\n",
    "for input_papers in tqdm(test_set):\n",
    "    result = {}\n",
    "    result['input'] = [input_paper['id'] for input_paper in input_papers]\n",
    "    \n",
    "    subgraph = expand_query(citation_graph, input_papers)    \n",
    "    result['output'] = recommend(subgraph, input_papers)\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOC_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63991143bac64c68842dc01ef4253d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GloDiSCern\n",
    "results = []\n",
    "\n",
    "for input_papers in tqdm(test_set):\n",
    "    result = {}\n",
    "    result['input'] = [input_paper['id'] for input_paper in input_papers]\n",
    "    \n",
    "    candidate_nodes = set()\n",
    "    for input_paper in input_papers:\n",
    "        for keyword in input_paper['fos']:\n",
    "            candidate_nodes.update(keyword_paper_dict[keyword])\n",
    "                \n",
    "    result['output'] = recommend(citation_graph.subgraph(candidate_nodes), input_papers)\n",
    "        \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(GLO_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
